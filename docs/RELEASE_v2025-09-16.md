# Release Report: v2025.09.16

## Release Summary
- Status: Stable freeze of existing infrastructure functions; no new feature work included in this cut.
- Scope: Validated Ollama GPU acceleration, Open WebUI responsiveness, and baseline Docker Compose stack operations.
- Audit Window: 16 Sep 2025 (local verification on Windows 11 Pro for Workstations).

## Validation & Coverage Overview
- Manual Checks: Open WebUI reachable at `http://localhost:3000` with responsive chat flows; Ollama models execute on available GPUs via `NVIDIA_VISIBLE_DEVICES=all`.
- Automation: No unit or integration test suites currently configured; coverage tooling is absent. Context sweep scripts are prepared but not executed during this release (recommended next run: `./scripts/context-sweep.ps1 -CpuOnly -Safe -WriteReport`).
- Risk Notes: Absence of automated regression tests; rely on manual smoke verification and compose health checks (`docker compose -f infra/compose/docker-compose.yml ps`).

## Issues & Resolutions
- Resolved: GPU enablement confirmed through Ollama container startup; no throttling observed while serving `llama3.1:8b` workloads.
- Resolved: Compose images pinned to `ollama/ollama:0.3.14`, `ghcr.io/open-webui/open-webui:v0.3.7`, and `qdrant/qdrant:v1.15.4` for deterministic rollbacks.
- Outstanding: Monitor upstream security patches and schedule validation windows before bumping the pinned compose tags.
- Watchlist: Qdrant persistence depends on existing `data/qdrant` volume; ensure backups before pruning containers.

## Dependency & Image Matrix
- Ollama Runtime: `ollama/ollama:0.3.14` (GPU-enabled; volumes mapped to `models/` cache and `modelfiles/`).
- Open WebUI: `ghcr.io/open-webui/open-webui:v0.3.7` (auth toggle via `OPENWEBUI_AUTH`).
- Qdrant Vector DB: `qdrant/qdrant:v1.15.4` storing data under `data/qdrant`.
- Scripts: PowerShell helpers under `scripts/` (compose, model lifecycle, evaluation).
- NVIDIA Toolkit: Accessed via container runtime; ensure host drivers match CUDA requirements (no container image modifications in this release).

## Configuration Snapshot
- Environment Variables: ports `WEBUI_PORT=3000`, `OLLAMA_PORT=11434`, `QDRANT_PORT=6333`; storage paths `MODELS_DIR=./models`, `DATA_DIR=./data`; authentication flag `OPENWEBUI_AUTH=false` (toggle before exposing beyond localhost).
- Compose Overrides: GPU access declared by `gpus: all` and `NVIDIA_DRIVER_CAPABILITIES=compute,utility` for the Ollama service.
- Network Bindings: Localhost endpoints — WebUI `3000/tcp`, Ollama `11434/tcp`, Qdrant `6333/tcp`.
- Persistence: Model cache under `models/` (git-ignored), service state under `data/` with directory-per-service segmentation.

## Operational Checklist
1. Bootstrap environment file: `Copy-Item .env.example .env` and adjust ports as needed.
2. Start stack: `./scripts/compose.ps1 up` (uses `infra/compose/docker-compose.yml`).
3. Verify services: `docker compose -f infra/compose/docker-compose.yml logs --tail 100`.
4. Model management: `./scripts/model.ps1 list` or `./scripts/model.ps1 pull -Model llama3.1:8b`.
5. Optional evaluation (post-stabilization): `./scripts/eval-context.ps1 -Model llama31-8b-c8k -TokensTarget 6000`.

## Next Steps
- Introduce automated smoke tests to quantify coverage and reduce manual verification load.
- Capture GPU utilization metrics (`nvidia-smi` logs) during long-running inference to track performance regressions.
- Document recovery procedures for Qdrant and WebUI data volumes prior to any container pruning cycles.
- Establish a cadence to review pinned image tags and apply security updates after validation.






